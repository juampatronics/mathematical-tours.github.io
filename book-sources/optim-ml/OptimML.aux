\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Motivation in Machine Learning}{2}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Unconstraint optimization}{2}{subsection.1.1}\protected@file@percent }
\newlabel{eq-general-pbm}{{1}{2}{Unconstraint optimization}{equation.1.1}{}}
\newlabel{eq-general-pbm-min}{{2}{2}{Unconstraint optimization}{equation.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  Left: linear regression, middle: linear classifier, right: loss function for classification. }}{3}{figure.1}\protected@file@percent }
\newlabel{fig-ml-ex}{{1}{3}{Left: linear regression, middle: linear classifier, right: loss function for classification}{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  Left: non-existence of minimizer, middle: multiple minimizers, right: uniqueness. }}{3}{figure.2}\protected@file@percent }
\newlabel{fig-minimizer-exists}{{2}{3}{Left: non-existence of minimizer, middle: multiple minimizers, right: uniqueness}{figure.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Regression}{3}{subsection.1.2}\protected@file@percent }
\newlabel{eq-least-square}{{3}{3}{Regression}{equation.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Classification}{3}{subsection.1.3}\protected@file@percent }
\newlabel{eq-classif}{{4}{3}{Classification}{equation.1.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Basics of Convex Analysis}{3}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Existence of Solutions}{3}{subsection.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  Coercivity condition for least squares. }}{4}{figure.3}\protected@file@percent }
\newlabel{fig-least-square}{{3}{4}{Coercivity condition for least squares}{figure.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Convexity}{4}{subsection.2.2}\protected@file@percent }
\newlabel{eq-convexity-def}{{5}{4}{Convexity}{equation.2.5}{}}
\@writefile{toc}{\contentsline {paragraph}{Strict convexity.}{4}{section*.2}\protected@file@percent }
\newlabel{eq-strict-convexity-def}{{6}{4}{Strict convexity}{equation.2.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  Convex vs. non-convex functions ; Strictly convex vs. non strictly convex functions. }}{5}{figure.4}\protected@file@percent }
\newlabel{fig-cvx-vs-noncvx}{{4}{5}{Convex vs. non-convex functions ; Strictly convex vs. non strictly convex functions}{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces  Comparison of convex functions $f : \mathbb  {R}^p \rightarrow \mathbb  {R}$ (for $p=1$) and convex sets $C \subset \mathbb  {R}^p$ (for $p=2$). }}{5}{figure.5}\protected@file@percent }
\newlabel{fig-cvx-set}{{5}{5}{Comparison of convex functions $f : \RR ^p \rightarrow \RR $ (for $p=1$) and convex sets $C \subset \RR ^p$ (for $p=2$)}{figure.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Convex Sets}{5}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Derivative and gradient}{5}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Gradient}{5}{subsection.3.1}\protected@file@percent }
\newlabel{eq-grad-dfn}{{7}{6}{Gradient}{equation.3.7}{}}
\newlabel{prop-above-tgt}{{1}{6}{}{prop.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}First Order Conditions}{6}{subsection.3.2}\protected@file@percent }
\newlabel{prop-cs-min}{{2}{6}{}{prop.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces  Function with local maxima/minima (left), saddle point (middle) and global minimum (right). }}{7}{figure.6}\protected@file@percent }
\newlabel{fig-first-order}{{6}{7}{Function with local maxima/minima (left), saddle point (middle) and global minimum (right)}{figure.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Least Squares}{7}{subsection.3.3}\protected@file@percent }
\newlabel{eq-grad-ls}{{8}{8}{Least Squares}{equation.3.8}{}}
\newlabel{eq-sol-leastsquare}{{9}{8}{Least Squares}{equation.3.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Link with PCA}{8}{subsection.3.4}\protected@file@percent }
\newlabel{eq-pca-decomp}{{10}{8}{Link with PCA}{equation.3.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces  Left: point clouds $(a_i)_i$ with associated PCA directions, right: quadratic part of $f(x)$. }}{9}{figure.7}\protected@file@percent }
\newlabel{fig-link-pca}{{7}{9}{Left: point clouds $(a_i)_i$ with associated PCA directions, right: quadratic part of $f(x)$}{figure.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Classification}{9}{subsection.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6}Chain Rule}{9}{subsection.3.6}\protected@file@percent }
\newlabel{eq-grad-composition-linear}{{11}{9}{Chain Rule}{equation.3.11}{}}
\newlabel{eq-differential-defn}{{12}{10}{Chain Rule}{equation.3.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Gradient Descent Algorithm}{10}{section.4}\protected@file@percent }
\newlabel{sec-grad-desc-basic}{{4}{10}{Gradient Descent Algorithm}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Steepest Descent Direction}{10}{subsection.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces  Left: First order Taylor expansion in 1-D and 2-D. Right: orthogonality of gradient and level sets and schematic of the proof. }}{11}{figure.8}\protected@file@percent }
\newlabel{fig-expansion-taylor}{{8}{11}{Left: First order Taylor expansion in 1-D and 2-D. Right: orthogonality of gradient and level sets and schematic of the proof}{figure.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Gradient Descent}{11}{subsection.4.2}\protected@file@percent }
\newlabel{eq-grad-desc}{{13}{11}{Gradient Descent}{equation.4.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces  Influence of $\tau $ on the gradient descent (left) and optimal step size choice (right). }}{12}{figure.9}\protected@file@percent }
\newlabel{fig-gradesc}{{9}{12}{Influence of $\tau $ on the gradient descent (left) and optimal step size choice (right)}{figure.9}{}}
\newlabel{eq-armijo-rule}{{14}{12}{Armijo rule}{equation.4.14}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Convergence Analysis}{12}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Quadratic Case}{12}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Convergence analysis for the quadratic case.}{12}{section*.3}\protected@file@percent }
\newlabel{prop-graddesc-quad}{{4}{13}{}{prop.4}{}}
\newlabel{eq-global-linrate-grad}{{15}{13}{}{equation.5.15}{}}
\newlabel{eq-best-rate-local}{{16}{13}{}{equation.5.16}{}}
\newlabel{eq-rate-strong-quad}{{17}{13}{Convergence analysis for the quadratic case}{equation.5.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces  Contraction constant $h(\tau )$ for a quadratic function (right). }}{14}{figure.10}\protected@file@percent }
\newlabel{fig-grad-desc-contract}{{10}{14}{Contraction constant $h(\tau )$ for a quadratic function (right)}{figure.10}{}}
\newlabel{prop-graddesc-quad-sublin}{{5}{14}{}{prop.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}General Case}{15}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Hessian.}{15}{section*.4}\protected@file@percent }
\newlabel{eq-taylor-hess}{{18}{15}{Hessian}{equation.5.18}{}}
\@writefile{toc}{\contentsline {paragraph}{Smoothness and strong convexity.}{16}{section*.5}\protected@file@percent }
\newlabel{eq-lipsch-grad}{{{$\mathcal  {R}_L$}}{16}{Smoothness and strong convexity}{equation.5.19}{}}
\newlabel{eq-strong-conv}{{{$\mathcal  {S}_\mu $}}{16}{Smoothness and strong convexity}{equation.5.19}{}}
\newlabel{prop-smooth-strong}{{6}{16}{}{prop.6}{}}
\newlabel{eq-above-below-quad}{{19}{16}{}{equation.5.19}{}}
\newlabel{eq-upper-lower-bound-hess}{{20}{16}{}{equation.5.20}{}}
\@writefile{toc}{\contentsline {paragraph}{Convergence analysis.}{17}{section*.6}\protected@file@percent }
\newlabel{thm-gradsec-non-strong-conv}{{1}{17}{}{thm.1}{}}
\newlabel{eq-sublin-rate-gd}{{21}{17}{}{equation.5.21}{}}
\newlabel{eq-proox-x'rad-nonstrong-1}{{22}{17}{Convergence analysis}{equation.5.22}{}}
\newlabel{eq-conv-rate-proof-1}{{25}{18}{Convergence analysis}{equation.5.25}{}}
\newlabel{eq-rate-strong}{{26}{18}{Convergence analysis}{equation.5.26}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Acceleration}{18}{subsection.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Mirror Descent and Implicit Bias}{19}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Bregman Divergences}{19}{subsection.6.1}\protected@file@percent }
\newlabel{eq-burg-entropy}{{27}{20}{Bregman Divergences}{equation.6.27}{}}
\newlabel{eq-power-entropies}{{28}{20}{Bregman Divergences}{equation.6.28}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Mirror descent}{20}{subsection.6.2}\protected@file@percent }
\newlabel{eq-mirror-descent}{{29}{21}{Mirror descent}{equation.6.29}{}}
\newlabel{eq-mirror-dual}{{30}{21}{Mirror descent}{equation.6.30}{}}
\@writefile{toc}{\contentsline {paragraph}{Mirror flow.}{21}{section*.7}\protected@file@percent }
\newlabel{eq-hessian-flow}{{31}{21}{Mirror flow}{equation.6.31}{}}
\@writefile{toc}{\contentsline {paragraph}{Convergence.}{21}{section*.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Re-parameterized flows}{21}{subsection.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Dual parameterization}{22}{section*.9}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Example: power-type parameterization}{22}{section*.10}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Counter-example: SDP matrices}{22}{section*.11}\protected@file@percent }
\newlabel{eq-flow-repar-matrices}{{32}{22}{Counter-example: SDP matrices}{equation.6.32}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Implicit Bias}{22}{subsection.6.4}\protected@file@percent }
\citation{gunasekar2018characterizing}
\citation{hoff2017lasso}
\citation{vavskevivcius2019implicit}
\citation{zhao2019implicit}
\citation{woodworth2020kernel}
\newlabel{prop-implicit-bias-l2}{{7}{23}{}{prop.7}{}}
\newlabel{prop-implicit-bias-mirror}{{8}{23}{}{prop.8}{}}
\newlabel{eq-implicit-bias-mirror}{{33}{23}{}{equation.6.33}{}}
\newlabel{eq-evol-dual}{{34}{23}{Implicit Bias}{equation.6.34}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Regularization}{23}{section.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Penalized Least Squares}{23}{subsection.7.1}\protected@file@percent }
\newlabel{eq-regul-ls}{{35}{23}{Penalized Least Squares}{equation.7.35}{}}
\newlabel{eq-regul-constr}{{36}{24}{}{equation.7.36}{}}
\newlabel{eq-ineq-proof-regul}{{37}{24}{Penalized Least Squares}{equation.7.37}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Ridge Regression}{24}{subsection.7.2}\protected@file@percent }
\newlabel{eq-regul-ls-1}{{38}{24}{}{equation.7.38}{}}
\newlabel{eq-regul-ls-2}{{39}{24}{}{equation.7.39}{}}
\@writefile{toc}{\contentsline {paragraph}{Pseudo-inverse.}{24}{section*.12}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces $\ell ^q$ balls $ \left \{ x \tmspace  +\thickmuskip {.2777em};\tmspace  +\thickmuskip {.2777em} \DOTSB \sum@ \slimits@ _k |x_k|^q \leqslant 1 \right \} $ for varying $q$. }}{25}{figure.11}\protected@file@percent }
\newlabel{fig-sparsity-lq}{{11}{25}{$\ell ^q$ balls $\enscond {x}{\sum _k |x_k|^q \leq 1}$ for varying $q$}{figure.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Lasso}{25}{subsection.7.3}\protected@file@percent }
\newlabel{prop-soft-tresdh}{{11}{25}{}{prop.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Evolution with $\lambda $ of the function $F(x) \ensuremath  {\stackrel  {\mbox  {\upshape \tiny  def.}}{=}}\frac  {1}{2}|\!| \cdot -y |\!|^2+\lambda |\cdot |$. }}{26}{figure.12}\protected@file@percent }
\newlabel{fig-varspars}{{12}{26}{Evolution with $\la $ of the function $F(x) \eqdef \frac {1}{2}\norm {\cdot -y}^2+\la |\cdot |$}{figure.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4}Iterative Soft Thresholding}{26}{subsection.7.4}\protected@file@percent }
\newlabel{sec-ista}{{7.4}{26}{Iterative Soft Thresholding}{subsection.7.4}{}}
\newlabel{eq-ista-surrog}{{40}{26}{Iterative Soft Thresholding}{equation.7.40}{}}
\newlabel{eq-ista}{{41}{26}{}{equation.7.41}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Stochastic Optimization}{27}{section.8}\protected@file@percent }
\newlabel{sec-stochastic-optim}{{8}{27}{Stochastic Optimization}{section.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Minimizing Sums and Expectation}{27}{subsection.8.1}\protected@file@percent }
\newlabel{eq-min-sums}{{42}{27}{Minimizing Sums and Expectation}{equation.8.42}{}}
\newlabel{eq-min-int}{{43}{27}{Minimizing Sums and Expectation}{equation.8.43}{}}
\newlabel{eq-stochastic-erm}{{44}{27}{Minimizing Sums and Expectation}{equation.8.44}{}}
\newlabel{eq-stoch-logistic}{{45}{27}{Minimizing Sums and Expectation}{equation.8.45}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Batch Gradient Descent (BGD)}{27}{subsection.8.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces  Evolution of the error of the BGD for logistic classification. }}{28}{figure.13}\protected@file@percent }
\newlabel{fig-bgd}{{13}{28}{Evolution of the error of the BGD for logistic classification}{figure.13}{}}
\newlabel{eq-full-grad}{{46}{28}{Batch Gradient Descent (BGD)}{equation.8.46}{}}
\newlabel{eq-grad-formula}{{47}{28}{Batch Gradient Descent (BGD)}{equation.8.47}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3}Stochastic Gradient Descent (SGD)}{28}{subsection.8.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Unbiased gradient estimate}}{28}{figure.14}\protected@file@percent }
\newlabel{eq-unbiased-grad}{{48}{28}{Stochastic Gradient Descent (SGD)}{equation.8.48}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces  Display of a large number of trajectories $k \DOTSB \mapstochar \rightarrow x_k \in \mathbb  {R}$ generated by several runs of SGD. On the top row, each curve is a trajectory, and the bottom row displays the corresponding density. }}{29}{figure.16}\protected@file@percent }
\newlabel{fig-sgd-traject}{{16}{29}{Display of a large number of trajectories $k \mapsto x_k \in \RR $ generated by several runs of SGD. On the top row, each curve is a trajectory, and the bottom row displays the corresponding density}{figure.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Schematic view of SGD iterates}}{29}{figure.15}\protected@file@percent }
\newlabel{eq-stepsize-sgd}{{49}{29}{Stochastic Gradient Descent (SGD)}{equation.8.49}{}}
\newlabel{thm-conv-sgd}{{2}{29}{}{thm.2}{}}
\newlabel{eq-rate-sgd}{{50}{29}{}{equation.8.50}{}}
\newlabel{eq-sgd-proof-1}{{51}{29}{Stochastic Gradient Descent (SGD)}{equation.8.51}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces  Evolution of the error of the SGD for logistic classification (dashed line shows BGD). }}{30}{figure.17}\protected@file@percent }
\newlabel{fig-sgd}{{17}{30}{Evolution of the error of the SGD for logistic classification (dashed line shows BGD)}{figure.17}{}}
\newlabel{eq-sgd-proof-2}{{52}{30}{Stochastic Gradient Descent (SGD)}{equation.8.52}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4}Stochastic Gradient Descent with Averaging (SGA)}{30}{subsection.8.4}\protected@file@percent }
\newlabel{sec-sga}{{8.4}{30}{Stochastic Gradient Descent with Averaging (SGA)}{subsection.8.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5}Stochastic Averaged Gradient Descent (SAG)}{31}{subsection.8.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9}Automatic Differentiation}{31}{section.9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces  Evolution of $\qopname  \relax o{log}_{10}(f(x_k)-f(x^\star ))$ for SGD, SGA and SAG. }}{32}{figure.18}\protected@file@percent }
\newlabel{fig-compariso-sgd}{{18}{32}{Evolution of $\log _{10}(f(x_k)-f(x^\star ))$ for SGD, SGA and SAG}{figure.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces  A computational graph. }}{32}{figure.19}\protected@file@percent }
\newlabel{fig-compgraph}{{19}{32}{A computational graph}{figure.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1}Finite Differences and Symbolic Calculus}{32}{subsection.9.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2}Computational Graphs}{32}{subsection.9.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces  Relation between the variable for the forward (left) and backward (right) modes. }}{33}{figure.20}\protected@file@percent }
\newlabel{fig-forward-backward}{{20}{33}{Relation between the variable for the forward (left) and backward (right) modes}{figure.20}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3}Forward Mode of Automatic Differentiation}{33}{subsection.9.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Simple example.}{33}{section*.13}\protected@file@percent }
\newlabel{eq-simple-func-autodiff}{{53}{33}{Simple example}{equation.9.53}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces  Example of a simple computational graph. }}{34}{figure.21}\protected@file@percent }
\newlabel{fig-dag-example-simple}{{21}{34}{Example of a simple computational graph}{figure.21}{}}
\@writefile{toc}{\contentsline {paragraph}{Dual numbers.}{34}{section*.14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4}Reverse Mode of Automatic Differentiation}{35}{subsection.9.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Back-propagation.}{35}{section*.15}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces  Complexity of forward (left) and backward (right) modes for composition of functions. }}{36}{figure.22}\protected@file@percent }
\newlabel{fig-matrix-mult}{{22}{36}{Complexity of forward (left) and backward (right) modes for composition of functions}{figure.22}{}}
\@writefile{toc}{\contentsline {paragraph}{Simple example.}{36}{section*.16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.5}Feed-forward Compositions}{36}{subsection.9.5}\protected@file@percent }
\newlabel{eq-simple-lin-dag}{{54}{36}{Feed-forward Compositions}{equation.9.54}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces  Computational graph for a feedforward architecture. }}{37}{figure.23}\protected@file@percent }
\newlabel{fig-mlp}{{23}{37}{Computational graph for a feedforward architecture}{figure.23}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.6}Feed-forward Architecture}{37}{subsection.9.6}\protected@file@percent }
\newlabel{eq-feednets}{{55}{37}{Feed-forward Architecture}{equation.9.55}{}}
\newlabel{eq-loss-feedf}{{56}{37}{Feed-forward Architecture}{equation.9.56}{}}
\newlabel{eq-backprop-discr}{{57}{37}{Feed-forward Architecture}{equation.9.57}{}}
\@writefile{toc}{\contentsline {paragraph}{Multilayers perceptron.}{37}{section*.17}\protected@file@percent }
\newlabel{eq-mlp-func}{{58}{37}{Multilayers perceptron}{equation.9.58}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces  Multi-layer perceptron parameterization. }}{38}{figure.24}\protected@file@percent }
\newlabel{fig-mlp-param}{{24}{38}{Multi-layer perceptron parameterization}{figure.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces  Computational graph for a recurrent architecture. }}{38}{figure.25}\protected@file@percent }
\newlabel{fig-recur}{{25}{38}{Computational graph for a recurrent architecture}{figure.25}{}}
\@writefile{toc}{\contentsline {paragraph}{Link with adjoint state method.}{38}{section*.18}\protected@file@percent }
\newlabel{eq-flow-eq}{{59}{38}{Link with adjoint state method}{equation.9.59}{}}
\newlabel{eq-ode-structure}{{60}{38}{Link with adjoint state method}{equation.9.60}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.7}Recurrent Architectures}{38}{subsection.9.7}\protected@file@percent }
\newlabel{eq-feednets-recur}{{61}{38}{Recurrent Architectures}{equation.9.61}{}}
\newlabel{eq-backprop-discr}{{62}{38}{Recurrent Architectures}{equation.9.62}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces  Recurrent residual perceptron parameterization. }}{39}{figure.26}\protected@file@percent }
\newlabel{fig-recurrent-param}{{26}{39}{Recurrent residual perceptron parameterization}{figure.26}{}}
\newlabel{eq-jacobian-mlp}{{63}{39}{Recurrent Architectures}{equation.9.63}{}}
\@writefile{toc}{\contentsline {paragraph}{Residual recurrent networks. }{39}{section*.19}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Mitigating memory requirement. }{39}{section*.20}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Fixed point maps}{40}{section*.21}\protected@file@percent }
\newlabel{eq-impl-func-formula}{{64}{40}{Fixed point maps}{equation.9.64}{}}
\@writefile{toc}{\contentsline {paragraph}{Argmin layers}{40}{section*.22}\protected@file@percent }
\newlabel{eq-argmin-layer}{{65}{40}{Argmin layers}{equation.9.65}{}}
\newlabel{eq-danskin}{{66}{40}{Argmin layers}{equation.9.66}{}}
\@writefile{toc}{\contentsline {paragraph}{Sinkhorn's algorithm}{40}{section*.23}\protected@file@percent }
\bibstyle{plain}
\bibdata{all}
\gdef \@abspage@last{41}
