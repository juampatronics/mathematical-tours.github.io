The Legendre-Fenchel transform f^* of a function f is convex. But the transformation is also jointly convex of both f and x.

Brascamp-Lieb and Cramér-Rao inequalities respectively lower and upper bound the variance of a random vector using the hessian of the log density.
https://en.wikipedia.org/wiki/Brascamp%E2%80%93Lieb_inequality
https://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93Rao_bound


Intriguing properties of the log-sum-exp (soft-max), which follow from Hölder’s and Prékopa’s inequalities. Learned it from @VesseronNina https://arxiv.org/abs/1304.0630 https://en.wikipedia.org/wiki/Pr%C3%A9kopa%E2%80%93Leindler_inequality

Under appropriate hypotheses, convexity is preserved by max and min marginalization. 

@article{li2020fourier,
  title={Fourier neural operator for parametric partial differential equations},
  author={Li, Zongyi and Kovachki, Nikola and Azizzadenesheli, Kamyar and Liu, Burigede and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
  journal={arXiv preprint arXiv:2010.08895},
  year={2020}
}

Stein's unbiased risk estimate (SURE) is an almost magical formula that enables the computation of the mean squared error of a denoiser (used, for example, in denoising score matching) using only the noisy observation y, without requiring the clean data x. https://en.wikipedia.org/wiki/Stein%27s_unbiased_risk_estimate

Using the SURE, it is straightforward to derive Tweedie’s formula for the optimal denoiser as the score function (the gradient of the log-likelihood of the noisy data), which is used in diffusion models. 